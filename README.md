# Evaluation Out-of-Context Object Recognition in Vision Transformers and CNNs

<a id="overview"></a> 
## 1. Project overview

In this project, the out-of-context (OOC) object recognition dataset generated using the toolkit from the novel [OOC Dataset Creator](https://github.com/ToastyDom/OOC-Dataset-Creator) [[1]](#1) is used to conduct experiments on OOC performance and translation invariance across three model families: Convolutional Neural Networks (CNNs), Vision Transformers (ViTs) and hybrid architectures. The project provides supporting scripts for conducting the experiments and visualizing the results presented in my bachelor thesis titled **Evaluating Out-of-Context Object Recognition in Vision Transformers and CNNs**.

The experiments conducted in this project include:

- Performance on OOC Dataset vs. baseline (ImageNet-1K Validation Set [[2]](#2))
- Performance on Background Set alone
- Performance on Similarity Ranked Compositions
- Translation Invariance with Systematic Placement Compositions

<a id="dataset"></a> 
## 2. Datasets
In this project, the following compositions in OOC Dataset are employed:
- OOC Compositions including hand-placed and automated Sets `datasets/ooc` or `datasets/OOC_Dataset/04_`
- Background Set `datasets/OOC_Dataset/02_backgrounds`
- Similarity Ranked Compositions `datasets/OOC_Dataset/04_OOC_compositions/similarity_ranked_compositions`
- Systematic Placement Compositions `datasets/OOC_Dataset/04_OOC_compositions/systematic_placements`

The folder of `OOC_Dataset` follows the folder structure in the repository [OOC-Dataset-Creator](https://github.com/ToastyDom/OOC-Dataset-Creator)

The script `dataset.py` is responsible for processing images in dataset before being passed into models for inference

```
# directory structure of datasets used in the experiments
root/                      # project root
└── datasets
    ├── ImageNet2012       # ImageNet2012 Validation Set
    ├── ooc                # Summary of handplaced and automated OOC Dataset 
    └── OOC_Dataset        # OOC Datset generated by toolkit
        ├── 00_helper_files
        ├── 01_ImageNet-S-919
        ├── 02_backgrounds
        ├── 03_object_cutouts
        └── 04_OOC_compositions
            ├── automatic_compositions
            ├── handplaced_compositions
            ├── handplaced_large_compositions
            ├── similarity_ranked_compositions
            └── systematic_placements
```

<a id="model"></a> 
## 3. Models
For the experiments, there are 11 CNN models, 18 ViT models, and 6 Hybrid models selected. All models used are from `timm` library [[3]](#3)). The detailed list of models is below. 

Other models for further experiments can be added in the script `models.py`, where the models are loaded for the inference.

| CNNs  | ViTs | Hybrids |
| --- | --- | --- |
| resnet34  | vit_small_patch16_224 | convnext_small |
| resnet50  | vit_small_patch32_224 | convnext_base |
| resnet101 | vit_base_patch16_224 | convnext_large |
| ssl_resnet50 | vit_base_patch32_224 | convnext_xlarge |
| inception_v3 | vit_large_patch16_224 | convnextv2_base |
| inception_v4 | vit_large_patch32_384 | convnextv2_large |
| efficientnetv2_rw_s | beit_base_patch16_224 |  |
| efficientnetv2_rw_m | beit_large_patch16_224 |  |
| efficientnet_b2 | beitv2_base_patch16_224 |  |
| efficientnet_b3 | beitv2_large_patch16_224 |  |
| efficientnet_b4 | beit3_base_patch16_224 |  |
|  | beit3_large_patch16_224 |  |
|  | deit_small_patch16_224 |  |
|  | deit_base_patch16_224 |  |
|  | deit3_small_patch16_224 |  |
|  | deit3_medium_patch16_224 |  |
|  | deit3_base_patch16_224 |  |
|  | deit3_large_patch16_224 |  |


<a id="inference"></a> 
## 4. Inference
The inference pipeline can be executed by the script `inference.py`. The settings used in the experiments are: batch size = 100, number of workers = 96
```
python inference.py -m "vit" -n 96 -b 100
```
Setting arguments:
- `-m`: model types (`cnn`, `vit`, `hybrid`)
- `-n`: number of workers
- `-b`: batch size

For inference on ImageNet-1K, argument `-t "imagenet"` should be involved, which is set as default when no parameter is passed into `inference()`)
```python
inference()
```

For inference on OOC set, similarity ranked compositions, and systematic placement compositions, it requires the data path and output path to be given. The data paths for the composition sets for the experiments in this project are listed in [2. Datasets](#dataset) 
```python
inference(data_path, output_path)
```

<a id="output"></a> 
## 5. Outputs
Top-1 and Top-5 accuracies are stored in `csv` and `json` files for inference on every datasets.

For inference on OOC compositions, predictions are explicitly stored in `csv` for further investigation and result visualization

Output path for inference:
- On ImageNet-1K Val Set: `outputs/accuracy_on_imagenet`.
- On background set: `outputs/background`
- On handplaced and automated OOC set: `outputs/accuracy_on_ooc`.
- On similarity ranked composition: `outputs/similarity_ranked`.
- On similarity ranked composition: `outputs/systematic_placements_6x6` and `outputs/systematic_placements_7x7` for experiment on `6x6` and `7x7` grid, respectively.

<a id="plot"></a> 
## 6. Plots
The plots visualizing the results are created through the corresponding script and stored under path as follows: 
- On ImageNet-1K Val Set: `plots.py` and  `plots/imagenet_vs_ooc`.
- On handplaced and automated OOC Set: `plots.py` and  `plots/imagenet_vs_ooc`.
- On background set: `plots_background_only.py` and  `plots/background`.
- On similarity ranked composition: `plots_similarity_ranked.py` and  `plots/similarity_ranked`.
- On similarity ranked composition: `plots_systematic_placement.py` and  `plots/systematic_placements`.

<a id="reference"></a> 
## 7. References
<a id="1">[1]</a> 
Domenic Bersch, Hai Van Tran. OOC Dataset Creator. [https://github.com/ToastyDom/OOC-Dataset-Creator](https://github.com/ToastyDom/OOC-Dataset-Creator). Commit b925922. 2025.

<a id="2">[2]</a> 
Olga Russakovsky et al. "Imagenet large scale visual recognition challenge." International journal of computer vision 115.3 (2015): 211-252.

<a id="3">[3]</a> 
Ross Wightman. PyTorch Image Models. Version 1.0.11. DOI: 10.5281/zenodo.4414861. URL: [https://github.com/huggingface/pytorch-image-models](https://github.com/huggingface/pytorch-image-models)
