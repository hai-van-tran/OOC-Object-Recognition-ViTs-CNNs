import pandas as pd
from pathlib import Path
import xml.etree.ElementTree as ET
import os
import json

import torch
import torch.nn as nn
from pprintpp import pprint


def summarize_accuracy():
    # read data from each file
    model_types = ['cnn', 'vit', 'hybrid']
    json_files = [os.path.join('outputs', model_type + '_accuracy.json') for model_type in model_types]
    all_data = {}

    for json_file in json_files:
        with open(json_file, 'r+', encoding='utf-8') as file:
            data = json.load(file)
            all_data.update(data)

    # add all data into a single json file
    all_json_file = os.path.join('outputs/accuracy.json')

    if not os.path.exists(all_json_file):
        with open(all_json_file, 'w', encoding='utf-8') as file:
            json.dump(all_data, file, indent=4)
    else:
        with open(all_json_file, 'r+', encoding='utf-8') as file:
            data = json.load(file)
            for key in all_data.keys():
                if not key in data:
                    data[key] = all_data[key]
            file.seek(0)
            json.dump(data, file, indent=4)
            file.truncate()


def add_class_labels():
    background_path = Path('datasets/create_ooc_dataset/backgrounds')
    csv_path = Path('datasets/create_ooc_dataset/class_labels_added.csv')
    bbox_path = Path('datasets/ImageNet2012/bbox/val')
    mapping_path = Path('datasets/ImageNet2012/LOC_synset_mapping.txt')

    # read current csv file
    df = pd.read_csv(csv_path)
    df['image_id'] = df['image_id'].astype(str)

    # read txt file
    with open(mapping_path) as file:
        text = file.read()
    text = text.split('\n')
    class_labels = {text[i][0:9] : text[i][10:].split(',')[0].replace(' ', '_') for i in range(len(text))}

    with open(csv_path, 'a') as file:
        file.write('\n')

    count = 0
    for img_path in sorted(background_path.glob('*.JPEG')):
        img_name = img_path.name
        if img_name in df['image_name'].tolist():
            continue

        anno_path = bbox_path / Path(img_path.stem + '.xml')
        root = ET.parse(anno_path).getroot()
        class_hash = root.findall('object/name')[0].text
        class_id = list(class_labels).index(class_hash)
        class_label = class_labels[class_hash]

        # write new row in csv file
        row = pd.DataFrame(
            {
                'class_hash': [class_hash],
                'image_name': [img_name],
                'image_id': [class_id],
                'class_label': [class_label]
            }
        )
        row.to_csv(csv_path, index=False, header=False, mode='a')

        count += 1

    print(count)


def create_metadata_ooc():
    class_label_path = Path('datasets/create_ooc_dataset/class_labels_added.csv')
    metadata_path = Path('datasets/ooc/dataset_metadata.json')
    ooc_path = Path('datasets/ooc/')

    df = pd.read_csv(class_label_path)

    # create json file if it does not exist
    if not metadata_path.exists():
        with open(metadata_path, 'w', encoding='utf-8') as file:
            json.dump([], file, indent=4)

    metadata_list = []
    # get metadata of each image
    for image_file in sorted(ooc_path.glob('*.png')):
        # image generated by code
        if '_auto_' in image_file.name:
            creation_method = 'auto'
            extracted_info = image_file.stem.rsplit('_', 3)[0].rsplit('_', 1)

        else:
            # image created manually
            creation_method = 'manual'
            extracted_info = image_file.stem.rsplit('_', 2)[0].rsplit('_', 1)
        background_image = extracted_info[0][:23]
        object_class = extracted_info[0].replace(background_image + '_', '')
        similarity_type = extracted_info[1]
        # get background class
        background_label = df[df['image_name'] == background_image + '.JPEG'].iloc[0]
        background_class_id = background_label['image_id']
        background_class = background_label['class_label']

        # exception: potter_s_wheel -> potter's_wheel,
        #            yellow_lady_s_slipper -> yellow_lady's_slipper
        #            carpenter_s_kit -> carpenter's_kit
        if '_s_' in object_class:
            object_class = object_class.replace("_s_", "'s_")
        # get object class id
        object_class_id = df[df['class_label'] == object_class].iloc[0]['image_id']

        metadata_dict = {
            'filename': image_file.name,
            'background_image': background_image + '.JPEG',
            'background_class_id': int(background_class_id),
            'background_class': background_class,
            'object_class_id': int(object_class_id),
            'object_class': object_class,
            'similarity_type': similarity_type,
            'creation_method': creation_method
        }
        metadata_list.append(metadata_dict)

    # add metadata into json file
    with open(metadata_path, 'r+', encoding='utf-8') as file:
        file.seek(0)
        json.dump(metadata_list, file, indent=4)
        file.truncate()


def save_prediction_ooc_dataset(output_root, data_path, model_name, img_names, pred_scores):
    class_labels_path = Path('datasets/create_ooc_dataset/class_labels_added.csv')
    metadata_path = data_path / 'dataset_metadata.json'
    output_path = output_root / 'ooc_predictions'

    # create output path if it does not exist
    output_path.mkdir(parents=True, exist_ok=True)

    # calculate probability distribution
    softmax = nn.Softmax(dim=1)
    probability_scores = softmax(pred_scores)

    # get prediction
    predictions = torch.max(probability_scores, 1)
    prediction_probabilities = predictions.values.tolist()
    prediction_class_ids = predictions.indices.tolist()

    # get all class labels
    class_label_df = pd.read_csv(class_labels_path).loc[:, ['image_id', 'class_label']].drop_duplicates()

    # read metadata of ooc images
    with open(metadata_path, 'r', encoding='utf-8') as file:
        data = json.load(file)
        df = pd.DataFrame(data)
        metadata = df[df['filename'].isin(img_names)].copy()

    # create output file if it does not exist
    prediction_path = output_path / Path(model_name + '.csv')
    columns = list(df) + [ 'prediction_class_id',
                           'prediction_class',
                           'prediction_probability',
                           'top_5_prediction_class_id',
                           'prediction_scores']

    if not prediction_path.exists():
        csv_df = pd.DataFrame(columns=columns)
        csv_df.to_csv(prediction_path, index=False)
    else:
        csv_df = pd.read_csv(prediction_path)

    for img, class_id, prob, score in zip(img_names, prediction_class_ids, prediction_probabilities, pred_scores):
        if img in csv_df.values:
            metadata = metadata.drop(metadata[metadata['filename'] == img].index)
            continue
        pred_class = class_label_df[class_label_df['image_id'] == class_id].iloc[0]['class_label']
        row = metadata[metadata['filename'] == img]
        metadata.loc[row.index, 'prediction_class_id'] = str(class_id)
        metadata.loc[row.index, 'prediction_class'] = pred_class
        metadata.loc[row.index, 'prediction_probability'] = round(prob, 5)
        metadata.loc[row.index, 'top_5_prediction_class_id'] = str(torch.topk(score, k=5).indices.tolist())
        metadata.loc[row.index, 'prediction_scores'] = str(score.tolist())

    if not metadata.empty:
        metadata.to_csv(prediction_path, index=False, header=False, mode='a')

if __name__ == '__main__':
    # summarize_accuracy()
    # add_class_labels()
    # create_metadata_ooc()
    pass