import csv

import pandas as pd
import numpy as np
from pathlib import Path
import xml.etree.ElementTree as ET
import os
import json

import torch
import torch.nn as nn
from pprintpp import pprint

import models

# path to helper files
IMAGENET_CLASS_INDEX = Path("datasets/OOC_Dataset/00_helper_files/imagenet_class_index.json")
SEMANTIC_SIMILARITY_MATRIX = Path("datasets/OOC_Dataset/00_helper_files/semantic_similarity_matrix.npy")

def summarize_accuracy():
    # read data from each file
    model_types = ['cnn', 'vit', 'hybrid']
    json_files = [os.path.join('outputs', model_type + '_accuracy.json') for model_type in model_types]
    all_data = {}

    for json_file in json_files:
        with open(json_file, 'r+', encoding='utf-8') as file:
            data = json.load(file)
            all_data.update(data)

    # add all data into a single json file
    all_json_file = os.path.join('outputs/accuracy.json')

    if not os.path.exists(all_json_file):
        with open(all_json_file, 'w', encoding='utf-8') as file:
            json.dump(all_data, file, indent=4)
    else:
        with open(all_json_file, 'r+', encoding='utf-8') as file:
            data = json.load(file)
            for key in all_data.keys():
                if not key in data:
                    data[key] = all_data[key]
            file.seek(0)
            json.dump(data, file, indent=4)
            file.truncate()


def add_class_labels():
    background_path = Path('datasets/create_ooc_dataset/backgrounds')
    csv_path = Path('datasets/create_ooc_dataset/class_labels_added.csv')
    bbox_path = Path('datasets/ImageNet2012/bbox/val')
    mapping_path = Path('datasets/ImageNet2012/LOC_synset_mapping.txt')

    # read current csv file
    df = pd.read_csv(csv_path)
    df['image_id'] = df['image_id'].astype(str)

    # read txt file
    with open(mapping_path) as file:
        text = file.read()
    text = text.split('\n')
    class_labels = {text[i][0:9] : text[i][10:].split(',')[0].replace(' ', '_') for i in range(len(text))}

    with open(csv_path, 'a') as file:
        file.write('\n')

    count = 0
    for img_path in sorted(background_path.glob('*.JPEG')):
        img_name = img_path.name
        if img_name in df['image_name'].tolist():
            continue

        anno_path = bbox_path / Path(img_path.stem + '.xml')
        root = ET.parse(anno_path).getroot()
        class_hash = root.findall('object/name')[0].text
        class_id = list(class_labels).index(class_hash)
        class_label = class_labels[class_hash]

        # write new row in csv file
        row = pd.DataFrame(
            {
                'class_hash': [class_hash],
                'image_name': [img_name],
                'image_id': [class_id],
                'class_label': [class_label]
            }
        )
        row.to_csv(csv_path, index=False, header=False, mode='a')

        count += 1

    print(count)


def create_metadata_ooc():
    class_label_path = Path('datasets/create_ooc_dataset/class_labels_added.csv')
    metadata_path = Path('datasets/ooc/dataset_metadata.json')
    ooc_path = Path('datasets/ooc/')

    df = pd.read_csv(class_label_path)

    # create json file if it does not exist
    if not metadata_path.exists():
        with open(metadata_path, 'w', encoding='utf-8') as file:
            json.dump([], file, indent=4)

    metadata_list = []
    # get metadata of each image
    for image_file in sorted(ooc_path.glob('*.png')):
        # image generated by code
        if '_auto_' in image_file.name:
            creation_method = 'auto'
            extracted_info = image_file.stem.rsplit('_', 3)[0].rsplit('_', 1)

        else:
            # image created manually
            creation_method = 'manual'
            extracted_info = image_file.stem.rsplit('_', 2)[0].rsplit('_', 1)
        background_image = extracted_info[0][:23]
        object_class = extracted_info[0].replace(background_image + '_', '')
        similarity_type = extracted_info[1]
        # get background class
        background_label = df[df['image_name'] == background_image + '.JPEG'].iloc[0]
        background_class_id = background_label['image_id']
        background_class = background_label['class_label']

        # exception: potter_s_wheel -> potter's_wheel,
        #            yellow_lady_s_slipper -> yellow_lady's_slipper
        #            carpenter_s_kit -> carpenter's_kit
        if '_s_' in object_class:
            object_class = object_class.replace("_s_", "'s_")
        # get object class id
        object_class_id = df[df['class_label'] == object_class].iloc[0]['image_id']

        metadata_dict = {
            'filename': image_file.name,
            'background_image': background_image + '.JPEG',
            'background_class_id': int(background_class_id),
            'background_class': background_class,
            'object_class_id': int(object_class_id),
            'object_class': object_class,
            'similarity_type': similarity_type,
            'creation_method': creation_method
        }
        metadata_list.append(metadata_dict)

    # add metadata into json file
    with open(metadata_path, 'r+', encoding='utf-8') as file:
        file.seek(0)
        json.dump(metadata_list, file, indent=4)
        file.truncate()


def save_prediction_ooc_dataset(output_root, data_path, model_name, img_names, pred_scores):
    class_labels_path = Path('datasets/create_ooc_dataset/class_labels_added.csv')
    metadata_path = data_path / 'dataset_metadata.json'
    output_path = output_root / 'ooc_predictions'

    # create output path if it does not exist
    output_path.mkdir(parents=True, exist_ok=True)

    # calculate probability distribution
    softmax = nn.Softmax(dim=1)
    probability_scores = softmax(pred_scores)

    # get prediction
    predictions = torch.max(probability_scores, 1)
    prediction_probabilities = predictions.values.tolist()
    prediction_class_ids = predictions.indices.tolist()

    # get all class labels
    class_label_df = pd.read_csv(class_labels_path).loc[:, ['image_id', 'class_label']].drop_duplicates()

    # read metadata of ooc images
    with open(metadata_path, 'r', encoding='utf-8') as file:
        data = json.load(file)
        df = pd.DataFrame(data)
        metadata = df[df['filename'].isin(img_names)].copy()

    # create output file if it does not exist
    prediction_path = output_path / Path(model_name + '.csv')
    columns = list(df) + [ 'prediction_class_id',
                           'prediction_class',
                           'prediction_probability',
                           'top_5_prediction_class_id',
                           'prediction_scores']

    if not prediction_path.exists():
        csv_df = pd.DataFrame(columns=columns)
        csv_df.to_csv(prediction_path, index=False)
    else:
        csv_df = pd.read_csv(prediction_path)

    for img, class_id, prob, score in zip(img_names, prediction_class_ids, prediction_probabilities, pred_scores):
        if img in csv_df.values:
            metadata = metadata.drop(metadata[metadata['filename'] == img].index)
            continue
        pred_class = class_label_df[class_label_df['image_id'] == class_id].iloc[0]['class_label']
        row = metadata[metadata['filename'] == img]
        metadata.loc[row.index, 'prediction_class_id'] = str(class_id)
        metadata.loc[row.index, 'prediction_class'] = pred_class
        metadata.loc[row.index, 'prediction_probability'] = round(prob, 5)
        metadata.loc[row.index, 'top_5_prediction_class_id'] = str(torch.topk(score, k=5).indices.tolist())
        # metadata.loc[row.index, 'prediction_scores'] = str(score.tolist()) # large data

    if not metadata.empty:
        metadata.to_csv(prediction_path, index=False, header=False, mode='a')

def save_prediction(csv_file_path, metadata_file, task, image_names, pred_scores):
    """
    save the predictions of a model into csv file
    :param csv_file_path: Path -- path to the csv file
    :param metadata_file: Path -- path to the metadata file
    :param task: str -- task (dataset type) on which the inference is done
    :param image_names: list(str) -- list of image names
    :param pred_scores:
    :return:
    """
    # define columns for csv file
    column = []
    if task == "background":
        column = [
            "dataset_id",
            "background_class_index",
            "prediction_class_index",
            "prediction_probability",
            "prediction_distance"
        ]
    elif task == "ranked": # TODO
        column = [
            "dataset_id",
            "background_class_index",
            "object_class_index",
            "prediction_class_index"
        ]
    elif task == "placement": # TODO
        column = [
            "dataset_id",
            "background_class_index",
            "object_class_index",
            "prediction_class_index"
        ]

    # create file if not exists and add columns into file
    if not csv_file_path.exists() and column:
        csv_df = pd.DataFrame(columns=column)
        csv_df.to_csv(csv_file_path, index=False)
    else:
        csv_df = pd.read_csv(csv_file_path)

    # get metadata
    metadata = pd.read_csv(metadata_file)
    class_hash_list = [metadata[metadata["dataset_id"] == img_name].iloc[0]["class_hash"] for img_name in image_names]
    background_class_indices = find_class_index_by_class_hash(class_hash_list)

    # get prediction class index
    softmax = nn.Softmax(dim=1)
    probability_scores = softmax(pred_scores)
    predictions = torch.max(probability_scores, 1)
    prediction_probabilities = predictions.values.tolist()
    prediction_class_indices = predictions.indices.tolist()

    # find rank (distance) for background only task
    prediction_rank_list = []
    if task == "background":
        prediction_rank_list = [find_rank(idx_1, idx_2) for idx_1, idx_2 in zip(background_class_indices, prediction_class_indices)]

    # add results into csv file
    added_row_list = []
    for name, bg_idx, pred_idx, pred_prob, rank  in zip(image_names, background_class_indices, prediction_class_indices, prediction_probabilities, prediction_rank_list):
        if name in csv_df.values:
            continue
        new_row = {
            "dataset_id": name,
            "background_class_index":  bg_idx,
            "prediction_class_index": pred_idx,
            "prediction_probability": round(pred_prob, 5),
            "prediction_distance": rank
        }
        added_row_list.append(new_row)
    add_to_csv = pd.DataFrame(added_row_list)
    add_to_csv.to_csv(csv_file_path, index=False, header=False, mode="a")

def find_rank(class_index_1, class_index_2):
    """
    calculate how far class index 2 is to class index 1:
    :param class_index_1: int | str -- index of target class
    :param class_index_2: int | str -- index of prediction class
    :return: int -- the rank (distance) of prediction class to the target class
    """
    semantic_similarity_matrix = np.load(SEMANTIC_SIMILARITY_MATRIX)
    ranking = semantic_similarity_matrix[int(class_index_1)]
    sorted_ranking = np.argsort(ranking)
    rank = sorted_ranking.tolist().index(int(class_index_2))

    return rank

def create_accuracy_files_and_check_model_status(json_file_path, csv_file_path, model_name):
    """
    create accuracy csv and json files if they are not exist. Check if the accuracy of the model is already saved
    :param json_file_path: Path -- path to json file
    :param csv_file_path: Path --path to csv file
    :param model_name: str -- name of the model
    :return: boolean -- return True if the model is done, otherwise return False
    """
    # create accuracy files if they not exist
    if not json_file_path.exists():
        with open(json_file_path, 'w', encoding='utf-8') as file:
            json.dump({}, file, indent=4)
    if not csv_file_path.exists():
        with open(csv_file_path, 'w', newline='', encoding='utf-8') as file:
            writer = csv.writer(file, delimiter=',')
            writer.writerow(['', 'top_1_accuracy', 'top_5_accuracy'])

    # check if the model has already completed inference
    json_check = False
    csv_check = False
    with open(json_file_path, 'r', encoding='utf-8') as file:
        data = json.load(file)
        if model_name in data:
            json_check = True
    with open(json_file_path, 'r', newline='', encoding='utf-8') as file:
        reader = csv.reader(file, delimiter=',')
        for row in reader:
            if model_name in row:
                csv_check = True
    if json_check and csv_check:
        return True

    return False


def save_accuracy(json_file_path, csv_file_path, model_name, top_1_accuracy, top_5_accuracy, done):
    """
    save accuracy of a model into csv and json files
    :param json_file_path: str -- path to json file
    :param csv_file_path: str --path to csv file
    :param model_name: str -- name of the model
    :param top_1_accuracy: float -- top-1 accuracy
    :param top_5_accuracy: float -- top-5 accuracy
    :param done: bool -- True if the model is the last one in the list
    :return:
    """
    # save result into a json file
    average_accuracy = []
    with open(json_file_path, 'r+', encoding='utf-8') as file:
        data = json.load(file)
        if model_name not in data:
            data[model_name] = {'top_1_accuracy': top_1_accuracy, 'top_5_accuracy': top_5_accuracy}
            # if inference is done for all models, calculate average accuracy
            if done:
                average_accuracy.append(np.mean([acc["top_1_accuracy"] for acc in data.values()]))
                average_accuracy.append(np.mean([acc["top_5_accuracy"] for acc in data.values()]))
                data["average"] = {'top_1_accuracy': average_accuracy[0], 'top_5_accuracy': average_accuracy[1]}
            file.seek(0)
            json.dump(data, file, indent=4)
            file.truncate()

    # save into csv file
    with open(csv_file_path, 'r+', newline='', encoding='utf-8') as file:
        reader = csv.reader(file, delimiter=',')
        exists = [model_name in row for row in reader]
        if True not in exists:
            writer = csv.writer(file, delimiter=',')
            writer.writerow([model_name, top_1_accuracy, top_5_accuracy])
            if done:
                writer.writerow(["average", average_accuracy[0], average_accuracy[1]])


def find_class_index_by_class_hash(class_hash_list):
    """
    Return corresponding class index of each class hash in the list
    :param class_hash_list: list(str) -- list of class hashes
    :return: list(int) list of corresponding class indices
    """
    with open(IMAGENET_CLASS_INDEX, "r", encoding="utf-8") as file:
        data = json.load(file)
        hash_index_mapping_dict = {value[0]: int(key) for key, value in data.items()} # format: {class_hash: class_index}
        class_index_list = [hash_index_mapping_dict[class_hash] for class_hash in class_hash_list]

    return class_index_list


if __name__ == '__main__':
    # summarize_accuracy()
    # add_class_labels()
    # create_metadata_ooc()
    pass