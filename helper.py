import csv

import pandas as pd
import numpy as np
from pathlib import Path
import xml.etree.ElementTree as ET
import os
import json

import torch
import torch.nn as nn
from pprintpp import pprint

import models

# path to helper files
IMAGENET_CLASS_INDEX = Path("datasets/OOC_Dataset/00_helper_files/imagenet_class_index.json")
SEMANTIC_SIMILARITY_MATRIX = Path("datasets/OOC_Dataset/00_helper_files/semantic_similarity_matrix.npy")


def summarize_accuracy():
    # read data from each file
    model_types = ['cnn', 'vit', 'hybrid']
    json_files = [os.path.join('outputs', model_type + '_accuracy.json') for model_type in model_types]
    all_data = {}

    for json_file in json_files:
        with open(json_file, 'r+', encoding='utf-8') as file:
            data = json.load(file)
            all_data.update(data)

    # add all data into a single json file
    all_json_file = os.path.join('outputs/accuracy.json')

    if not os.path.exists(all_json_file):
        with open(all_json_file, 'w', encoding='utf-8') as file:
            json.dump(all_data, file, indent=4)
    else:
        with open(all_json_file, 'r+', encoding='utf-8') as file:
            data = json.load(file)
            for key in all_data.keys():
                if not key in data:
                    data[key] = all_data[key]
            file.seek(0)
            json.dump(data, file, indent=4)
            file.truncate()


def add_class_labels():
    background_path = Path('datasets/create_ooc_dataset/backgrounds')
    csv_path = Path('datasets/create_ooc_dataset/class_labels_added.csv')
    bbox_path = Path('datasets/ImageNet2012/bbox/val')
    mapping_path = Path('datasets/ImageNet2012/LOC_synset_mapping.txt')

    # read current csv file
    df = pd.read_csv(csv_path)
    df['image_id'] = df['image_id'].astype(str)

    # read txt file
    with open(mapping_path) as file:
        text = file.read()
    text = text.split('\n')
    class_labels = {text[i][0:9]: text[i][10:].split(',')[0].replace(' ', '_') for i in range(len(text))}

    with open(csv_path, 'a') as file:
        file.write('\n')

    count = 0
    for img_path in sorted(background_path.glob('*.JPEG')):
        img_name = img_path.name
        if img_name in df['image_name'].tolist():
            continue

        anno_path = bbox_path / Path(img_path.stem + '.xml')
        root = ET.parse(anno_path).getroot()
        class_hash = root.findall('object/name')[0].text
        class_id = list(class_labels).index(class_hash)
        class_label = class_labels[class_hash]

        # write new row in csv file
        row = pd.DataFrame(
            {
                'class_hash': [class_hash],
                'image_name': [img_name],
                'image_id': [class_id],
                'class_label': [class_label]
            }
        )
        row.to_csv(csv_path, index=False, header=False, mode='a')

        count += 1

    print(count)


def create_metadata_ooc():
    class_label_path = Path('datasets/create_ooc_dataset/class_labels_added.csv')
    metadata_path = Path('datasets/ooc/dataset_metadata.json')
    ooc_path = Path('datasets/ooc/')

    df = pd.read_csv(class_label_path)

    # create json file if it does not exist
    if not metadata_path.exists():
        with open(metadata_path, 'w', encoding='utf-8') as file:
            json.dump([], file, indent=4)

    metadata_list = []
    # get metadata of each image
    for image_file in sorted(ooc_path.glob('*.png')):
        # image generated by code
        if '_auto_' in image_file.name:
            creation_method = 'auto'
            extracted_info = image_file.stem.rsplit('_', 3)[0].rsplit('_', 1)

        else:
            # image created manually
            creation_method = 'manual'
            extracted_info = image_file.stem.rsplit('_', 2)[0].rsplit('_', 1)
        background_image = extracted_info[0][:23]
        object_class = extracted_info[0].replace(background_image + '_', '')
        similarity_type = extracted_info[1]
        # get background class
        background_label = df[df['image_name'] == background_image + '.JPEG'].iloc[0]
        background_class_id = background_label['image_id']
        background_class = background_label['class_label']

        # exception: potter_s_wheel -> potter's_wheel,
        #            yellow_lady_s_slipper -> yellow_lady's_slipper
        #            carpenter_s_kit -> carpenter's_kit
        if '_s_' in object_class:
            object_class = object_class.replace("_s_", "'s_")
        # get object class id
        object_class_id = df[df['class_label'] == object_class].iloc[0]['image_id']

        metadata_dict = {
            'filename': image_file.name,
            'background_image': background_image + '.JPEG',
            'background_class_id': int(background_class_id),
            'background_class': background_class,
            'object_class_id': int(object_class_id),
            'object_class': object_class,
            'similarity_type': similarity_type,
            'creation_method': creation_method
        }
        metadata_list.append(metadata_dict)

    # add metadata into json file
    with open(metadata_path, 'r+', encoding='utf-8') as file:
        file.seek(0)
        json.dump(metadata_list, file, indent=4)
        file.truncate()


def save_prediction_ooc_dataset(output_root, data_path, model_name, img_names, pred_scores):
    class_labels_path = Path('datasets/create_ooc_dataset/class_labels_added.csv')
    metadata_path = data_path / 'dataset_metadata.json'
    output_path = output_root / 'ooc_predictions'

    # create output path if it does not exist
    output_path.mkdir(parents=True, exist_ok=True)

    # calculate probability distribution
    softmax = nn.Softmax(dim=1)
    probability_scores = softmax(pred_scores)

    # get prediction
    predictions = torch.max(probability_scores, 1)
    prediction_probabilities = predictions.values.tolist()
    prediction_class_ids = predictions.indices.tolist()

    # get all class labels
    class_label_df = pd.read_csv(class_labels_path).loc[:, ['image_id', 'class_label']].drop_duplicates()

    # read metadata of ooc images
    with open(metadata_path, 'r', encoding='utf-8') as file:
        data = json.load(file)
        df = pd.DataFrame(data)
        metadata = df[df['filename'].isin(img_names)].copy()

    # create output file if it does not exist
    prediction_path = output_path / Path(model_name + '.csv')
    columns = list(df) + ['prediction_class_id',
                          'prediction_class',
                          'prediction_probability',
                          'top_5_prediction_class_id',
                          'prediction_scores']

    if not prediction_path.exists():
        csv_df = pd.DataFrame(columns=columns)
        csv_df.to_csv(prediction_path, index=False)
    else:
        csv_df = pd.read_csv(prediction_path)

    for img, class_id, prob, score in zip(img_names, prediction_class_ids, prediction_probabilities, pred_scores):
        if img in csv_df.values:
            metadata = metadata.drop(metadata[metadata['filename'] == img].index)
            continue
        pred_class = class_label_df[class_label_df['image_id'] == class_id].iloc[0]['class_label']
        row = metadata[metadata['filename'] == img]
        metadata.loc[row.index, 'prediction_class_id'] = str(class_id)
        metadata.loc[row.index, 'prediction_class'] = pred_class
        metadata.loc[row.index, 'prediction_probability'] = round(prob, 5)
        metadata.loc[row.index, 'top_5_prediction_class_id'] = str(torch.topk(score, k=5).indices.tolist())
        # metadata.loc[row.index, 'prediction_scores'] = str(score.tolist()) # large data

    if not metadata.empty:
        metadata.to_csv(prediction_path, index=False, header=False, mode='a')


def save_prediction(csv_file_path, metadata_file, task, image_names, labels, pred_scores):
    """
    save the predictions of a model into csv file
    :param csv_file_path: Path -- path to the csv file
    :param metadata_file: Path -- path to the metadata file
    :param task: str -- task (dataset type) on which the inference is done
    :param image_names: list(str) -- list of image names
    :param labels: list(int) -- list of labels
    :param pred_scores:
    :return:
    """
    # define columns for csv file
    column = [
        "dataset_id",
        "actual_class_index",
        "prediction_class_index",
        "prediction_probability",
        "prediction_distance"
    ]

    # create file if not exists and add columns into file
    if not csv_file_path.exists() and column:
        csv_df = pd.DataFrame(columns=column)
        csv_df.to_csv(csv_file_path, index=False)
    else:
        csv_df = pd.read_csv(csv_file_path)

    # get metadata
    metadata = pd.read_csv(metadata_file)

    # get background class index:
    # class_hash_list = [metadata[metadata["dataset_id"] == img_name].iloc[0]["class_hash"] for img_name in image_names]
    # background_class_indices = find_class_index_by_class_hash(class_hash_list)

    # get prediction class index
    softmax = nn.Softmax(dim=1)
    probability_scores = softmax(pred_scores)
    predictions = torch.max(probability_scores, 1)
    prediction_probabilities = predictions.values.tolist()
    prediction_class_indices = predictions.indices.tolist()

    # find rank (distance) of prediction to actual class
    prediction_rank_list = [find_rank(idx_1, idx_2) for idx_1, idx_2 in zip(labels, prediction_class_indices)]

    # add results into csv file
    added_row_list = []
    for name, label_idx, pred_idx, pred_prob, rank in zip(image_names, labels, prediction_class_indices,
                                                          prediction_probabilities, prediction_rank_list):
        if name in csv_df.values:
            continue
        new_row = {
            "dataset_id": name,
            "actual_class_index": label_idx,
            "prediction_class_index": pred_idx,
            "prediction_probability": round(pred_prob, 5),
            "prediction_distance": rank
        }
        added_row_list.append(new_row)
    add_to_csv = pd.DataFrame(added_row_list)
    add_to_csv.to_csv(csv_file_path, index=False, header=False, mode="a")


def find_rank(class_index_1, class_index_2):
    """
    calculate how far class index 2 is to class index 1:
    :param class_index_1: int | str -- index of target class
    :param class_index_2: int | str -- index of prediction class
    :return: int -- the rank (distance) of prediction class to the target class
    """
    semantic_similarity_matrix = np.load(SEMANTIC_SIMILARITY_MATRIX)
    ranking = semantic_similarity_matrix[int(class_index_1)]
    sorted_ranking = np.argsort(ranking)
    rank = sorted_ranking.tolist().index(int(class_index_2))

    return rank


def create_accuracy_files_and_check_model_status(json_file_path, csv_file_path, model_name):
    """
    create accuracy csv and json files if they are not exist. Check if the accuracy of the model is already saved
    :param json_file_path: Path -- path to json file
    :param csv_file_path: Path --path to csv file
    :param model_name: str -- name of the model
    :return: boolean -- return True if the model is done, otherwise return False
    """
    # create accuracy files if they not exist
    if not json_file_path.exists():
        with open(json_file_path, 'w', encoding='utf-8') as file:
            json.dump({}, file, indent=4)
    if not csv_file_path.exists():
        with open(csv_file_path, 'w', newline='', encoding='utf-8') as file:
            writer = csv.writer(file, delimiter=',')
            writer.writerow(['', 'top_1_accuracy', 'top_5_accuracy'])

    # check if the model has already completed inference
    json_check = False
    csv_check = False
    with open(json_file_path, 'r', encoding='utf-8') as file:
        data = json.load(file)
        if model_name in data:
            json_check = True

    # with open(csv_file_path, 'r', newline='', encoding='utf-8') as file:
    #     reader = csv.reader(file, delimiter=',')
    #     for row in reader:
    #         if model_name in row:
    #             csv_check = True

    df = pd.read_csv(csv_file_path)
    if (df.iloc[:, 0].str.contains(model_name, na=False)).any():
        csv_check = True

    if json_check and csv_check:
        return True

    return False


def save_accuracy(json_file_path, csv_file_path, model_name, top_1_accuracy, top_5_accuracy, done):
    """
    save accuracy of a model into csv and json files
    :param json_file_path: str -- path to json file
    :param csv_file_path: str --path to csv file
    :param model_name: str -- name of the model
    :param top_1_accuracy: float -- top-1 accuracy
    :param top_5_accuracy: float -- top-5 accuracy
    :param done: bool -- True if the model is the last one in the list
    :return:
    """
    # save result into a json file
    average_accuracy = []
    with open(json_file_path, 'r+', encoding='utf-8') as file:
        data = json.load(file)
        if model_name not in data:
            data[model_name] = {'top_1_accuracy': top_1_accuracy, 'top_5_accuracy': top_5_accuracy}
            # if inference is done for all models, calculate average accuracy
            if done:
                average_accuracy.append(np.mean([acc["top_1_accuracy"] for acc in data.values()]))
                average_accuracy.append(np.mean([acc["top_5_accuracy"] for acc in data.values()]))
                data["average"] = {'top_1_accuracy': average_accuracy[0], 'top_5_accuracy': average_accuracy[1]}
            file.seek(0)
            json.dump(data, file, indent=4)
            file.truncate()

    # save into csv file
    with open(csv_file_path, 'r+', newline='', encoding='utf-8') as file:
        reader = csv.reader(file, delimiter=',')
        exists = [model_name in row for row in reader]
        if True not in exists:
            writer = csv.writer(file, delimiter=',')
            writer.writerow([model_name, top_1_accuracy, top_5_accuracy])
            if done:
                writer.writerow(["average", average_accuracy[0], average_accuracy[1]])


def find_class_index_by_class_hash(class_hash_list):
    """
    Return corresponding class index of each class hash in the list
    :param class_hash_list: list(str) -- list of class hashes
    :return: list(int) list of corresponding class indices
    """
    with open(IMAGENET_CLASS_INDEX, "r", encoding="utf-8") as file:
        data = json.load(file)
        hash_index_mapping_dict = {value[0]: int(key) for key, value in
                                   data.items()}  # format: {class_hash: class_index}
        class_index_list = [hash_index_mapping_dict[class_hash] for class_hash in class_hash_list]

    return class_index_list


def get_background_imagenet_s_list():
    """
    get list of background names that have segmentation mask in ImageNet-S
    :return: list(str)
    """
    # list of imagenet-s image names
    imagenet_s = Path("datasets/OOC_Dataset/01_ImageNet-S-919/imagenet-s_metadata.csv")
    s_df = pd.read_csv(imagenet_s)
    s_list = s_df["image_name"].tolist()

    # list of background names
    backgrounds = Path("datasets/OOC_Dataset/02_backgrounds/backgrounds_metadata.csv")
    bg_df = pd.read_csv(backgrounds)
    bg_list = bg_df["imagenet_name"].tolist()

    # get list of background names that exist in imagenet-s
    bg_s_list = []
    for bg in bg_list:
        if bg in s_list:
            bg_s_list.append(bg)

    # check if background images has segmentation masks
    seg_path = Path("datasets/OOC_Dataset/01_ImageNet-S-919/segmentation")
    bg_s_count = 0
    final_list = []

    for bg in bg_s_list:
        seg_file = seg_path / bg.replace("JPEG", "png")
        if seg_file.is_file():
            bg_s_count += 1
            final_list.append(bg)
    return final_list


def check_missing_outputs():
    """
    to check missing results
    """
    model_list = models.get_model_list("")
    data_root = Path("datasets/OOC_Dataset/04_OOC_compositions/similarity_ranked_compositions")
    output_root = Path("outputs/similarity_ranked")
    paths = output_root.glob("*")
    path_list = list(paths)

    count_cnn_missing = 0
    count_vit_missing = 0
    count_hybrid_missing = 0
    count_img_missing_pred = 0
    for p in path_list:
        # check folder accuracy, prediction
        if len(list(p.glob("*"))) == 2:
            a_path = p / "accuracy"
            p_path = p / "prediction"

            # check number of accuracy files
            if len(list(a_path.glob("*"))) < 6:
                for m in ["cnn", "vit", "hybrid"]:
                    if not list(a_path.glob(f"*{m}*")):
                        # print(f"{p} {m} accuracy missing")
                        if m == "cnn":
                            count_cnn_missing += 1
                        elif m == "vit":
                            count_vit_missing += 1
                        else:
                            count_hybrid_missing += 1

            # check number of prediction files
            if len(list(p_path.glob("*"))) < len(model_list):
                count_img_missing_pred += 1
                count_models = 0
                for m in model_list:
                    if not list(p_path.glob(f"*{m}*")):
                        count_models += 1
                        # print(f"{p} {m} prediction missing")
                # print(f"--> {p} {count_models} predictions missing")

            # check if the number of predictions in each model matches the number of data
            data_path = data_root / p.name / "images"
            count_data = len(list(data_path.glob("*")))
            for p_file in p_path.glob("*"):
                print(f"checking {p_file}")
                df = pd.read_csv(p_file)
                count_row = len(df)
                if count_row < count_data:
                    print(f"{p_file} missing {count_data - count_row} predictions")

        else:
            print(f"{p} missing a folder!")

    print("=====================================")
    print(f"path counts = 247: {len(path_list) == 247}")
    print(
        f"Accuracy files: {count_cnn_missing} cnn missing, {count_vit_missing} vit missing, {count_hybrid_missing} hybrid missing")
    print(f"Prediction files: {count_img_missing_pred} img folders missing some pred files")

def statistics_in_ooc_dataset():
    root = Path("datasets/OOC_Dataset")
    # count number of background image belonging to imagenet-s, number of classes, number of background types
    bg_path = root / "02_backgrounds/backgrounds_metadata.csv"
    imagenet_s_path = root / "01_ImageNet-S-919/imagenet-s_metadata.csv"
    mask_path = root / "01_ImageNet-S-919/segmentation"
    val_path = root / "01_ImageNet-S-919/validation"

    bg_df = pd.read_csv(bg_path, usecols=["imagenet_name", "class_hash", "background_type"])
    in_s_df = pd.read_csv(imagenet_s_path, usecols=["image_name"])

    num_bg_in_s = bg_df["imagenet_name"].isin(in_s_df["image_name"]).sum()

    num_bg_has_valid_mask = 0
    num_bg_in_val = 0
    for bg in bg_df["imagenet_name"]:
        mask = mask_path / bg.replace(".JPEG", ".png")
        val = val_path / bg
        if mask.is_file():
            num_bg_has_valid_mask += 1
        if val.is_file():
            num_bg_in_val += 1

    num_class_hash = bg_df["class_hash"].nunique()
    background_type_count = bg_df["background_type"].value_counts()

    # print(num_bg_in_s)
    # print(num_bg_has_valid_mask)
    # print(num_bg_in_val)
    # print(num_class_hash)
    # print(background_type_count)

    # object cutouts
    cutout_path = root / "03_object_cutouts"
    num_cutouts = sum(1 for p in cutout_path.iterdir() if p.is_dir())
    # print(num_cutouts)

    # OOC compositions
    ooc_auto_path = root / "04_OOC_compositions/automatic_compositions/automated_metadata.csv"
    ooc_handplaced_path = root / "04_OOC_compositions/handplaced_compositions/handplaced_metadata.csv"
    ooc_handplaced_l_path = root / "04_OOC_compositions/handplaced_large_compositions/backgrounds_metadata.csv"
    ooc_auto_df = pd.read_csv(ooc_auto_path, usecols=["dataset_id", "object_class_hash"])
    ooc_handplaced_df = pd.read_csv(ooc_handplaced_path, usecols=["dataset_id", "object_class_hash"])
    ooc_handplaced_l_df = pd.read_csv(ooc_handplaced_l_path, usecols=["dataset_id", "object_class_hash"])
    # print(f"{len(ooc_auto_df)} {len(ooc_handplaced_df)} {len(ooc_handplaced_l_df)}")

    ooc_auto_df["bg_name"] = ooc_auto_df["dataset_id"].str.split("_with_").str[0]
    ooc_handplaced_df["bg_name"] = ooc_handplaced_df["dataset_id"].str.split("_with_").str[0]
    ooc_handplaced_l_df["bg_name"] = ooc_handplaced_l_df["dataset_id"].str.split("_with_").str[0]
    combined_ooc_df = pd.concat([ooc_auto_df, ooc_handplaced_df, ooc_handplaced_l_df], ignore_index=True)
    # print(len(combined_ooc_df))
    num_usage_bg = combined_ooc_df["bg_name"].value_counts()
    # for bg, count in num_usage_bg.items():
    #     print((bg + ".JPEG") in in_s_df["image_name"].values)
    #     print(f"{bg} {count}")

    num_object_class_auto = ooc_auto_df.groupby("bg_name")["object_class_hash"].nunique()
    num_object_class_handplaced = ooc_handplaced_df.groupby("bg_name")["object_class_hash"].nunique()
    num_object_class_handplaced_l = ooc_handplaced_l_df.groupby("bg_name")["object_class_hash"].nunique()

    print(f"auto: {num_object_class_auto.min()} - {num_object_class_auto.max()}")
    print(f"hand: {num_object_class_handplaced.min()} - {num_object_class_handplaced.max()}")
    print(f"hand_l: {num_object_class_handplaced_l.min()} - {num_object_class_handplaced_l.max()}")


    # similarity ranked composition, systematic placement composition
    similarity_ranked_path = root / "04_OOC_compositions/similarity_ranked_compositions"
    bg_dir_list = list(similarity_ranked_path.glob("*"))
    num_bg = len(bg_dir_list)
    num_img = 0
    num_class_object_list = []
    count = 0
    for dir in bg_dir_list:
        img_path = dir / "images"
        count += 1
        print(f"{count}..scanning {img_path.name}..")

        num_img += sum(1 for p in img_path.iterdir() if p.is_file())

        simi_ranked_df = pd.read_csv(next(dir.glob("*metadata*.csv")), usecols=["similarity_rank"])
        num_object_class = simi_ranked_df.nunique()
        num_class_object_list.append(num_object_class.values[0])

    print(len(num_class_object_list))
    print(f"Min - Max: {min(num_class_object_list)} - {max(num_class_object_list)}")
    print(num_class_object_list)

    # print(similarity_ranked_path.name)
    # print(num_bg)
    # print(num_img)




    systematic_placement_6x6_path = root / "04_OOC_compositions/systematic_placements_6x6"
    bg_dir_list = list(systematic_placement_6x6_path.glob("*"))
    num_bg = len(bg_dir_list)
    num_img = 0
    for dir in bg_dir_list:
        img_path = dir / "images"
        num_img += sum(1 for p in img_path.iterdir() if p.is_file())
    # print(systematic_placement_6x6_path.name)
    # print(num_bg)
    # print(num_img)

    systematic_placement_7x7_path = root / "04_OOC_compositions/systematic_placements_7x7"
    bg_dir_list = list(systematic_placement_7x7_path.glob("*"))
    num_bg = len(bg_dir_list)
    num_img = 0
    for dir in bg_dir_list:
        img_path = dir / "images"
        num_img += sum(1 for p in img_path.iterdir() if p.is_file())
    # print(systematic_placement_7x7_path.name)
    # print(num_bg)
    # print(num_img)



if __name__ == '__main__':
    # summarize_accuracy()
    # add_class_labels()
    # create_metadata_ooc()
    # bg_list = get_background_imagenet_s_list()
    # pass
    check_missing_outputs()
    statistics_in_ooc_dataset()
